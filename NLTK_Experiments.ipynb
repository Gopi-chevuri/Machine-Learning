{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK Experiments",
      "provenance": [],
      "authorship_tag": "ABX9TyP3xMymjIlZ2TGnkzPU6j0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohansaidinesh/Machine-Learning/blob/main/NLTK_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 12"
      ],
      "metadata": {
        "id": "3jjS2U1WhRL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perform Noise Removal on text by writing appropriate functions for stopword removal and punctuation removal."
      ],
      "metadata": {
        "id": "2ANeiD2ckJc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zqOHdvrbuBr",
        "outputId": "3d4c5d77-f6db-4a88-8d5c-0b59cb955b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence : We are learning Natural Language Processing (NLP) as part of\n",
            "Fundamentals of Machine Learning (FML) course in our second year B.Tech.\n",
            "Sentence after removing punctuations : We are learning Natural Language Processing NLP as part of\n",
            "Fundamentals of Machine Learning FML course in our second year BTech\n",
            "Sentence after removing stopwords We learning Natural Language Processing NLP part Fundamentals Machine Learning FML course second year BTech\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "def remove_punctuation(sentence):\n",
        "  for i in sentence:\n",
        "    if i in string.punctuation:\n",
        "      sentence = sentence.replace(i, \"\")\n",
        "  return sentence\n",
        "def remove_stopwords(sentence):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(example_sent)\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "  return filtered_sentence\n",
        "example_sent = \"\"\"We are learning Natural Language Processing (NLP) as part of\n",
        "Fundamentals of Machine Learning (FML) course in our second year B.Tech.\"\"\"\n",
        "print(\"Original Sentence :\", example_sent)\n",
        "example_sent = remove_punctuation(example_sent)\n",
        "print(\"Sentence after removing punctuations :\",example_sent)\n",
        "example_sent = \" \".join(remove_stopwords(example_sent))\n",
        "print(\"Sentence after removing stopwords\",example_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment13"
      ],
      "metadata": {
        "id": "mtuYr0LShjc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization using Pythons split() function:\n"
      ],
      "metadata": {
        "id": "vieMNjKthvhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "text = '''Once there lived a greedy man in a small town. He was very rich, and he loved gold a\n",
        "nd all things fancy. But he loved his daughter more than anything. One day, he chanced upo\n",
        "n a fairy. The fairy’s hair was caught in a few tree branches. He helped her out, but as his gre\n",
        "ediness took over, he realised that he had an opportunity to become richer by asking for a w\n",
        "ish in return (by helping her out). The fairy granted him a wish. He said, “All that I touch sho\n",
        "uld turn to gold.” And his wish was granted by the grateful fairy.'''\n",
        "tokens = text.split()\n",
        "print(tokens)\n",
        "print(\"No.of tokens : \", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsq8SyVYh0r8",
        "outputId": "67e3dc9f-6cae-4fcd-cb6e-c29bdc49912c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town.', 'He', 'was', 'very', 'rich,', 'and', 'he', 'loved', 'gold', 'a', 'nd', 'all', 'things', 'fancy.', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything.', 'One', 'day,', 'he', 'chanced', 'upo', 'n', 'a', 'fairy.', 'The', 'fairy’s', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches.', 'He', 'helped', 'her', 'out,', 'but', 'as', 'his', 'gre', 'ediness', 'took', 'over,', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'w', 'ish', 'in', 'return', '(by', 'helping', 'her', 'out).', 'The', 'fairy', 'granted', 'him', 'a', 'wish.', 'He', 'said,', '“All', 'that', 'I', 'touch', 'sho', 'uld', 'turn', 'to', 'gold.”', 'And', 'his', 'wish', 'was', 'granted', 'by', 'the', 'grateful', 'fairy.']\n",
            "No.of tokens :  108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "text = '''Once there lived a greedy man in a small town. He was very rich, and he loved gold a\n",
        "nd all things fancy. But he loved his daughter more than anything. One day, he chanced upo\n",
        "n a fairy. The fairy’s hair was caught in a few tree branches. He helped her out, but as his gre\n",
        "ediness took over, he realised that he had an opportunity to become richer by asking for a w\n",
        "ish in return (by helping her out). The fairy granted him a wish. He said, “All that I touch sho\n",
        "uld turn to gold.” And his wish was granted by the grateful fairy.'''\n",
        "sentences = text.split('.')\n",
        "print(sentences)\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTvjUvFVh6mt",
        "outputId": "c13d4388-6891-4f1b-9aee-5243d3d5c4c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once there lived a greedy man in a small town', ' He was very rich, and he loved gold a\\nnd all things fancy', ' But he loved his daughter more than anything', ' One day, he chanced upo\\nn a fairy', ' The fairy’s hair was caught in a few tree branches', ' He helped her out, but as his gre\\nediness took over, he realised that he had an opportunity to become richer by asking for a w\\nish in return (by helping her out)', ' The fairy granted him a wish', ' He said, “All that I touch sho\\nuld turn to gold', '” And his wish was granted by the grateful fairy', '']\n",
            "No.of sentences :  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization using Regular Expressions (RegEx)\n"
      ],
      "metadata": {
        "id": "X6mw_aG4iA97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "import re\n",
        "tokens = re.findall(\"[\\w']+\", text)\n",
        "print(tokens)\n",
        "print(\"No.of tokens : \", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wchQrBuQiHED",
        "outputId": "d30c1912-b069-4d91-e0be-37e561bfc4e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town', 'He', 'was', 'very', 'rich', 'and', 'he', 'loved', 'gold', 'a', 'nd', 'all', 'things', 'fancy', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything', 'One', 'day', 'he', 'chanced', 'upo', 'n', 'a', 'fairy', 'The', 'fairy', 's', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches', 'He', 'helped', 'her', 'out', 'but', 'as', 'his', 'gre', 'ediness', 'took', 'over', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'w', 'ish', 'in', 'return', 'by', 'helping', 'her', 'out', 'The', 'fairy', 'granted', 'him', 'a', 'wish', 'He', 'said', 'All', 'that', 'I', 'touch', 'sho', 'uld', 'turn', 'to', 'gold', 'And', 'his', 'wish', 'was', 'granted', 'by', 'the', 'grateful', 'fairy']\n",
            "No.of tokens :  109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenization\n",
        "sentences = re.compile('[.?!] ').split(text)\n",
        "print(sentences)\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O-uKHtpiQJc",
        "outputId": "ec1732af-4c13-4ee5-c802-ee7fbec7e028"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once there lived a greedy man in a small town', 'He was very rich, and he loved gold a\\nnd all things fancy', 'But he loved his daughter more than anything', 'One day, he chanced upo\\nn a fairy', 'The fairy’s hair was caught in a few tree branches', 'He helped her out, but as his gre\\nediness took over, he realised that he had an opportunity to become richer by asking for a w\\nish in return (by helping her out)', 'The fairy granted him a wish', 'He said, “All that I touch sho\\nuld turn to gold.” And his wish was granted by the grateful fairy.']\n",
            "No.of sentences :  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization using NLTK"
      ],
      "metadata": {
        "id": "GrgmItRLiYqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3isNA7ri4qz",
        "outputId": "c2a51c21-7b2b-419b-f743-2f04615c59a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "print(\"No.of tokens : \", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq63aMWliazl",
        "outputId": "bfe6eb0f-18b7-4b4b-b179-0187cbf0188d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town', '.', 'He', 'was', 'very', 'rich', ',', 'and', 'he', 'loved', 'gold', 'a', 'nd', 'all', 'things', 'fancy', '.', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything', '.', 'One', 'day', ',', 'he', 'chanced', 'upo', 'n', 'a', 'fairy', '.', 'The', 'fairy', '’', 's', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches', '.', 'He', 'helped', 'her', 'out', ',', 'but', 'as', 'his', 'gre', 'ediness', 'took', 'over', ',', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'w', 'ish', 'in', 'return', '(', 'by', 'helping', 'her', 'out', ')', '.', 'The', 'fairy', 'granted', 'him', 'a', 'wish', '.', 'He', 'said', ',', '“', 'All', 'that', 'I', 'touch', 'sho', 'uld', 'turn', 'to', 'gold.', '”', 'And', 'his', 'wish', 'was', 'granted', 'by', 'the', 'grateful', 'fairy', '.']\n",
            "No.of tokens :  127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPVR0-Lii8gL",
        "outputId": "405f7f90-cff1-48c8-8cc3-83806f96db6d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once there lived a greedy man in a small town.', 'He was very rich, and he loved gold a\\nnd all things fancy.', 'But he loved his daughter more than anything.', 'One day, he chanced upo\\nn a fairy.', 'The fairy’s hair was caught in a few tree branches.', 'He helped her out, but as his gre\\nediness took over, he realised that he had an opportunity to become richer by asking for a w\\nish in return (by helping her out).', 'The fairy granted him a wish.', 'He said, “All that I touch sho\\nuld turn to gold.” And his wish was granted by the grateful fairy.']\n",
            "No.of sentences :  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 14"
      ],
      "metadata": {
        "id": "TNKMQMoGjAKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement a function which takes a sentence and returns the stemmed sentence"
      ],
      "metadata": {
        "id": "nMcsBC8BkRrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def stemSentence(sentence):\n",
        " token_words=word_tokenize(sentence)\n",
        " print(token_words)\n",
        " stem_sentence=[]\n",
        " for word in token_words:\n",
        "  stem_sentence.append(porter.stem(word))\n",
        "  stem_sentence.append(\" \")\n",
        " return \"\".join(stem_sentence)\n",
        "x=stemSentence(sentence)\n",
        "print(\"Sentence after stemming :\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_k5WLAAjD4l",
        "outputId": "47173ecf-284c-4065-f63f-60a0420f9253"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "Sentence after stemming : python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 15"
      ],
      "metadata": {
        "id": "Mj192oY_jX4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement a function which takes a sentence and returns the lemmatized sentence"
      ],
      "metadata": {
        "id": "A5FfGF8ykW3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def lemmatizeSentence(sentence):\n",
        " token_words=word_tokenize(sentence)\n",
        " print(token_words)\n",
        " lemma_sentence=[]\n",
        " for word in token_words:\n",
        "  lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
        "  lemma_sentence.append(\" \")\n",
        " return \"\".join(lemma_sentence)\n",
        "x=lemmatizeSentence(sentence)\n",
        "print(\"Sentence after Lemmatization :\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJqyWMH2jaQV",
        "outputId": "d84be985-4618-4e2c-d26a-8b8efb7bdd23"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
            "Sentence after Lemmatization : He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 16"
      ],
      "metadata": {
        "id": "YaO_xSnBj49K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create an N-gram language model by using Reuters corpus of the NLTK library"
      ],
      "metadata": {
        "id": "nzqyt31YkcIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "from nltk import trigrams\n",
        "from collections import defaultdict\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "for sentence in reuters.sents():\n",
        "  for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "    model[(w1, w2)][w3] += 1\n",
        "for w1_w2 in model:\n",
        "  total_count = float(sum(model[w1_w2].values()))\n",
        "  for w3 in model[w1_w2]:\n",
        "    model[w1_w2][w3] /= total_count\n",
        "print(dict(model['what','is']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10SG1gMUdA-_",
        "outputId": "ed4381f3-c521-45a4-9543-19def3d3f10b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'really': 0.05, 'sees': 0.05, 'happening': 0.1, 'due': 0.05, 'to': 0.05, 'important': 0.1, 'otherwise': 0.05, 'France': 0.05, 'produced': 0.05, 'now': 0.1, 'traditionally': 0.05, 'effectively': 0.05, 'seen': 0.05, 'required': 0.1, 'boosting': 0.1}\n"
          ]
        }
      ]
    }
  ]
}